{"cells":[{"cell_type":"markdown","source":["#### Names of people in the group\n\nPlease write the names of the people in your group in the next cell."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83045409-70a5-4010-b1cf-9db8d98f2bad"}}},{"cell_type":"markdown","source":["Name of person A Karl Edvin Undheim\n\nName of person B"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"059a269d-8f13-494e-84e5-9da5d873047b"}}},{"cell_type":"code","source":["# We need to install 'ipython_unittest' to run unittests in a Jupyter notebook\n!pip install -q ipython_unittest"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52939e3d-35c7-4767-a59e-ba75f716e952"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\u001B[33mWARNING: You are using pip version 21.0.1; however, version 22.0.4 is available.\r\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[33mWARNING: You are using pip version 21.0.1; however, version 22.0.4 is available.\r\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Loading modules that we need\nimport unittest\nfrom pyspark.sql.dataframe import DataFrame\nfrom typing import Any"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"499e46f5-12a4-46de-a212-90f2e936461e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# A helper function to load a table (stored in Parquet format) from DBFS as a Spark DataFrame \ndef load_df(table_name: \"name of the table to load\") -> DataFrame:\n    return spark.read.parquet(table_name)\n\nusers_df = load_df(\"/user/hive/warehouse/users\")\ncomments_df = load_df(\"/user/hive/warehouse/comments\")\nposts_df = load_df(\"/user/hive/warehouse/posts\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f8c68e0-6bda-40a6-8588-381e51dc0a93"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 1: implenenting two helper functions\nImpelment these two functions:\n1. 'run_query' that gets a Spark SQL query and run it on df which is a Spark DataFrame; it returns the content of the first column of the first row of the DataFrame that is the output of the query;\n2. 'run_query2' that is similar to 'run_query' but instead of one DataFrame gets two; it returns the content of the first column of the first row of the DataFrame that is the output of the query.\n\nNote that the result of a Spark SQL query is itself a Spark DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a2274f0-1bd8-4812-83d2-f793587e9548"}}},{"cell_type":"code","source":["def run_query(query: \"a SQL query string\", df: \"the DataFrame that the query will be executed on\") -> Any:\n  # tempview defines how to call the dataframe in the query\n  df.createOrReplaceTempView(\"df\");\n  sqlDf = spark.sql(query)\n  return sqlDf.first()[0]\n\ndef run_query2(query: \"a SQL query string\", df1: \"DataFrame A\", df2: \"DataFrame B\") -> Any:\n  # Same as previous function but here we have two different dataframes\n  df1.createOrReplaceTempView(\"df1\");\n  df2.createOrReplaceTempView(\"df2\");\n  sqlDf = spark.sql(query)\n  return sqlDf.first()[0]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5fce811-7dd2-4602-a243-18a36754c302"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Loading 'ipython_unittest' so we can use '%%unittest_main' magic command\n%load_ext ipython_unittest"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a021cce-880b-42b1-a09d-6d2c2222124e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"The ipython_unittest extension is already loaded. To reload it, use:\n  %reload_ext ipython_unittest\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["The ipython_unittest extension is already loaded. To reload it, use:\n  %reload_ext ipython_unittest\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 2: writing a few queries\nWrite the following queries in SQL to be executed by Spark in the next cell.\n\n1. 'q1': find the 'Id' of the most recently created post ('df' is 'posts_df') \n2. 'q2': find the number users\n3. 'q3': find the 'Id' of the user who posted most number of answers\n4. 'q4': find the number of questions\n5. 'q5': find the display name of the user who posted most number of comments\n\nNote that 'q1' is already available below as an example. Moreover, remmebr that Spark supports ANSI SQL 2003 so your queries have to comply with that standard."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7de10aba-7e77-4baa-af35-2b4e37916071"}}},{"cell_type":"code","source":["q1 = \"SELECT * FROM df ORDER BY CreationDate DESC limit 1\"\n\nq2 = \"SELECT COUNT(DISTINCT Id) FROM df\"\n\nq3 = \"SELECT df.OwnerUserId, COUNT(df.PostTypeId) AS AnswerCount \\\n      FROM df \\\n      WHERE df.PostTypeId==2 \\\n      GROUP BY df.OwnerUserId \\\n      ORDER BY AnswerCount DESC\"\n\nq4 = \"SELECT COUNT(PostTypeId) AS QuestionCount\\\n      FROM df\\\n      WHERE PostTypeId==1\"\n\n# Here I create a table containing each userId and how many comments this id has posted. This table must then be joined with the user table to be able to extract displayname.\nq5 = \"SELECT df1.DisplayName\\\n      FROM \\\n        (SELECT df2.UserId, COUNT(df2.UserId) AS AnswerCount \\\n        FROM df2 \\\n        GROUP BY df2.UserId \\\n        ORDER BY AnswerCount DESC \\\n        limit 1) C INNER JOIN df1 ON C.UserId==df1.Id\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3273ef7-7f4d-4b5d-bcf7-9935c952e26d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 3: validating the implementations by running the tests\n\nRun the cell below and make sure that all the tests run successfully."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46400bac-c11c-4bee-81a8-67d42e3ca968"}}},{"cell_type":"code","source":["%%unittest_main\nclass TestTask2(unittest.TestCase):\n    def test_q1(self):\n        # find the id of the most recent post\n        r = run_query(q1, posts_df)\n        self.assertEqual(r, 95045)\n\n    def test_q2(self):\n        # find the number of the users\n        r = run_query(q2, users_df)\n        self.assertEqual(r, 91616)\n\n    def test_q3(self):\n        # find the user id of the user who posted most number of answers\n        r = run_query(q3, posts_df)\n        self.assertEqual(r, 64377)\n\n    def test_q4(self):\n        # find the number of questions\n        r = run_query(q4, posts_df)\n        self.assertEqual(r, 28950)\n\n    def test_q5(self):\n        # find the display name of the user who posted most number of comments\n        r = run_query2(q5, users_df, comments_df)\n        self.assertEqual(r, \"Neil Slater\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31ca3fb9-427c-425d-b334-0df9c208a21b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Success.....\n----------------------------------------------------------------------\nRan 5 tests in 5.502s\n\nOK\nOut[14]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Success.....\n----------------------------------------------------------------------\nRan 5 tests in 5.502s\n\nOK\nOut[14]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 4: answering to questions about Spark related concepts\n\nPlease answer the following questions. Write your answer in one to two short paragraphs. Don't copy-paste; instead, write your own understanding.\n\n1. What is the difference between 'DataFrame', 'Dataset', and 'Resilient Distributed Datasets (RDD)'? \n2. When do you suggest using RDDs instead of using DataFrames?\n3. What is the main benefit of using DataSets instead of DataFrames?\n\nWrite your answers in the next cell."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe8a2e0c-7e72-410b-b385-00503c0bc136"}}},{"cell_type":"markdown","source":["1: Together they make up the three different spark APIs which can be used.\nAll of them are are immutable collections of objects which are partitioned across nodes in a cluster. They are all lazily evaluated and fault tolerant.\n- The original API used in spark. The RDD(Resilient Distributed Datasets) is unstructured, meaning there is no inferred schema for columns and rows. It is also type safe, meaning validation of datatypes happens during compiling, rather than during execution. \n- The DataFrame expands on the RDD mainly by providing named columns(structured), and supports operations such as aggregate, select and sum. However it is no longer type safe.\n- The Dataset expands on the dataframe and aims to combine parts from the RDD and dataframe. Notably type safety is provided aswell as many other features.\n\n2: RDDs could be used when you are working with unstructured data and you dont need to use a schema. Additionally if you only require low level transformations they are useful, as you dont need the operations provided by dataframes/datasets. This allows the user more control.\n\n3: The main benefit is that datasets support the object oriented interface and type safety used in the RDDs. This combines the best aspects of RDDs and dataframes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c33ca21-1672-4c0c-a876-f73cbb8f65b2"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Task2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4038497947934884}},"nbformat":4,"nbformat_minor":0}
