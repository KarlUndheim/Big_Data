{"cells":[{"cell_type":"markdown","source":["#### Names of people in the group\n\nPlease write the names of the people in your group in the next cell."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85d8965a-c0b4-4802-abb8-05fc5e21ce59"}}},{"cell_type":"markdown","source":["Name of person A Karl Edvin Undheim\n\nName of person B"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37c623c5-86c0-48ee-85de-5d514b4b334f"}}},{"cell_type":"code","source":["# Deleting tables left from previous runs in case they still exist after deleting an inactive cluster\ndbutils.fs.rm(\"/user\", recurse=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"324eb902-51f2-44b0-8ff1-730efac9900c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[1]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[1]: True"]}}],"execution_count":0},{"cell_type":"code","source":["# We need to install 'ipython_unittest' to run unittests in a Jupyter notebook\n!pip install -q ipython_unittest"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60b530d6-b580-4de2-affb-aad878f4da86"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\u001B[33mWARNING: You are using pip version 21.0.1; however, version 22.0.4 is available.\r\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[33mWARNING: You are using pip version 21.0.1; however, version 22.0.4 is available.\r\nYou should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Loading PySpark modules that we need\nimport unittest\nfrom collections import Counter\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.types import *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"023f4d10-e729-450c-8d1a-50d494b488d6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 1: defining the schema for the data\nTypically, the first thing to do before loading the data into a Spark cluster is to define the schema for the data. Look at the schema for 'badges' and try to define the schema for other tables similarly."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fcda919c-b51e-4b61-9d61-80cc24f2d15e"}}},{"cell_type":"code","source":["# In this task I used the \"discription of the data\" document we were provided to find each of the files' content. For some attributes I was unsure of its type. To clarify this I went to \"Data\" and created a table for the file which had the data I was unsure of. In this table it is clear which type it is. An example was the reputation in users, which I interpreted as either string or integer.\n# Next I defined the schemas as shown in badges_schema\n\n# Defining a schema for 'badges' table\nbadges_schema = StructType([StructField('UserId', IntegerType(), False),\n                            StructField('Name', StringType(), False),\n                            StructField('Date', TimestampType(), False),\n                            StructField('Class', IntegerType(), False)])\n\n# Defining a schema for 'posts' table\nposts_schema = StructType([StructField('Id', IntegerType(), False),\n                           StructField('ParentId', IntegerType(), False),\n                           StructField('PostTypeId', IntegerType(), False),\n                           StructField('CreationDate', TimestampType(), False),\n                           StructField('Score', IntegerType(), False),\n                           StructField('ViewCount', IntegerType(), False),\n                           StructField('Body', StringType(), False),\n                           StructField('OwnerUserId', IntegerType(), False),\n                           StructField('LastActivityDate', TimestampType(), False),\n                           StructField('Title', StringType(), False),\n                           StructField('Tags', StringType(), False),\n                           StructField('AnswerCount', IntegerType(), False),\n                           StructField('CommentCount', IntegerType(), False),\n                           StructField('FavoriteCount', IntegerType(), False),\n                           StructField('CloseDate', TimestampType(), False)])\n\n# Defining a schema for 'users' table\nusers_schema = StructType([StructField('Id', IntegerType(), False),\n                           StructField('Reputation', IntegerType(), False),\n                           StructField('CreationDate', TimestampType(), False),\n                           StructField('DisplayName', StringType(), False),\n                           StructField('LastAccessDate', TimestampType(), False),\n                           StructField('AboutMe', StringType(), False),\n                           StructField('Views', IntegerType(), False),\n                           StructField('UpVotes', IntegerType(), False),\n                           StructField('DownVotes', IntegerType(), False)])\n\n# Defining a schema for 'comments' table\ncomments_schema = StructType([StructField('PostId', IntegerType(), False),\n                          StructField('Score', IntegerType(), False),\n                          StructField('Text', StringType(), False),\n                          StructField('CreationDate', TimestampType(), False),\n                          StructField('UserId', IntegerType(), False)])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab8bdba5-f6d6-43bf-bed9-763d99cfcc91"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 2: implementing two helper functions\nNext, we need to implement two helper functions:\n1. 'load_csv' that as input argument receives path for a CSV file and a schema and loads the CSV pointed by the path into a Spark DataFrame and returns the DataFrame;\n2. 'save_df' receives a Spark DataFrame and saves it as a Parquet file on DBFS.\n\nNote that the column separator in CSV files is TAB character ('\\t') and the first row includes the name of the columns. \n\nBTW, DBFS is the name of the distributed filesystem used by Databricks Community Edition to store and access data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fab2cfe2-0961-4a22-8fb1-9a6f9191fbcf"}}},{"cell_type":"code","source":["def load_csv(source_file: \"path for the CSV file to load\", schema: \"schema for the CSV file being loaded as a DataFrame\") -> DataFrame:\n  # Here I used the spark operation to read from a csv file.\n  # delimiter defines the column separator, which is \\t as stated.\n  df = spark.read.load(source_file, format=\"csv\", delimiter='\\t', schema=schema, header=\"true\")\n  return df;\n\n\ndef save_df(df: \"DataFrame to be saved\", table_name: \"name under which the DataFrame will be saved\") -> None:\n  df.write.parquet(\"user/hive/warehouse/\"+table_name)\n  return None;\n    ## YOUR IMPLEMENTATION ##"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"089f87ff-f2c8-4ac8-8449-cec251c502f6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Loading 'ipython_unittest' so we can use '%%unittest_main' magic command\n%load_ext ipython_unittest"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"39bc683c-b37a-4842-8bf8-004620b17cca"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 3: validating the implementation by running the tests\n\nRun the cell below and make sure that all the tests run successfully. Moreover, at the end there should be four Parquet files named 'badges', 'comments', 'posts', and 'users' in '/user/hive/warehouse'.\n\nNote that we assumed that the data for the project has already been stored on DBFS on the '/FileStore/tables/' path. (I mean as 'badges_csv.gz', 'comments_csv.gz', 'posts_csv.gz', and 'users_csv.gz'.)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8903e903-7e4f-4c15-99bd-c9129a601fde"}}},{"cell_type":"code","source":["%%unittest_main\nclass TestTask1(unittest.TestCase):\n   \n    # test 1\n    def test_load_badges(self):\n        result = load_csv(source_file=\"/FileStore/tables/badges_csv.gz\", schema=badges_schema)\n        self.assertIsNotNone(result, \"Badges dataframe did not load successfully\")\n        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n        self.assertEqual(result.count(), 105640, \"Number of records is not correct\")\n\n        coulmn_names = Counter(map(str.lower, ['UserId', 'Name', 'Date', 'Class']))\n        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n                              \"Missing column(s) or column name mismatch\")\n    \n    # test 2\n    def test_load_posts(self):\n        result = load_csv(source_file=\"/FileStore/tables/posts_csv.gz\", schema=posts_schema)\n        self.assertIsNotNone(result, \"Posts dataframe did not load successfully\")\n        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n        self.assertEqual(result.count(), 61432, \"Number of records is not correct\")\n\n        coulmn_names = Counter(map(str.lower,\n                                   ['Id', 'ParentId', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'OwnerUserId',\n                                    'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount', 'FavoriteCount',\n                                    'CloseDate']))\n        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n                              \"Missing column(s) or column name mismatch\")\n    \n    # test 3\n    def test_load_comments(self):\n        result = load_csv(source_file=\"/FileStore/tables/comments_csv.gz\", schema=comments_schema)\n        self.assertIsNotNone(result, \"Comments dataframe did not load successfully\")\n        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n        self.assertEqual(result.count(), 58735, \"Number of records is not correct\")\n\n        coulmn_names = Counter(map(str.lower, ['PostId', 'Score', 'Text', 'CreationDate', 'UserId']))\n        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n                              \"Missing column(s) or column name mismatch\")\n    \n    # test 4\n    def test_load_users(self):\n        result = load_csv(source_file=\"/FileStore/tables/users_csv.gz\", schema=users_schema)\n        self.assertIsNotNone(result, \"Users dataframe did not load successfully\")\n        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n        self.assertEqual(result.count(), 91616, \"Number of records is not correct\")\n\n        coulmn_names = Counter(map(str.lower,\n                                   ['Id', 'Reputation', 'CreationDate', 'DisplayName', 'LastAccessDate', 'AboutMe',\n                                    'Views', 'UpVotes', 'DownVotes']))\n        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n                              \"Missing column(s) or column name mismatch\")\n    # test 5\n    def test_save_dfs(self):\n        dfs = [(\"/FileStore/tables/users_csv.gz\", users_schema, \"users\"),\n               (\"/FileStore/tables/badges_csv.gz\", badges_schema, \"badges\"),\n               (\"/FileStore/tables/comments_csv.gz\", comments_schema, \"comments\"),\n               (\"/FileStore/tables/posts_csv.gz\", posts_schema, \"posts\")\n               ]\n\n        for i in dfs:\n            df = load_csv(source_file=i[0], schema=i[1])\n            save_df(df, i[2])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd470d59-2571-4b7d-b022-9ee8f7c3e281"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Success.....\n----------------------------------------------------------------------\nRan 5 tests in 43.511s\n\nOK\nOut[7]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Success.....\n----------------------------------------------------------------------\nRan 5 tests in 43.511s\n\nOK\nOut[7]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40692338-44ce-43a7-a331-d60e1ba1918b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 4: answering to questions about Spark related concepts\n\nPlease write a short description for the terms below---one to two short paragraphs for each term. Don't copy-paste; instead, write your own understanding.\n\n1. What do the terms 'Spark Application', 'SparkSession', 'Transformations', 'Action', and 'Lazy Evaluation' mean in the context of Spark?\n\nWrite your descriptions in the next cell."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f99b257-8618-4796-aeb0-d9446863c259"}}},{"cell_type":"markdown","source":["Note: I will use the term spark datatype when reffering to RDD/DataFrame/Dataset\n\n- A spark application consists of everything involved with running the user's code to compute a result. There are 3 main parts: The driver process, executors and the cluster manager. The driver process consists of a spark session and the user code. This process communicates with the executors and the cluster manager. The executors only execute the code given to them and return the result. The cluster manager oversees spark applications and allocates resources to them.\n\n- The spark session is a connection to a spark cluster. It processes the users code and delegates work to the executors (computing etc). Thus the sparksession provides support for different features such as creating dataframes and performing SQL queries.\n\n- Transformations in spark are operations which return one of the spark datatypes( RDD, DataFrame or dataset). In spark these data structures are immutable which means a transformation will create a new structure, instead of updating the previous one. In short a transformation takes a spark dataype as input and returns a spark datatype as output. Example: RDD -> RDD.\n\n- Actions differ from transformations in that they do not return a spark datatype. Rather they return a value/object of some sort. This value is then returned to the driver process where it can be used further. Example: RDD -> object\n\n- Lazy evaluation means that a spark datatype wont be computed until it is used in an action. This means that if you have a chain of transformations, the process wont execute until an action is called. The advantage of waiting is that spark can then create an optimal way to compute the result by removing or merging redundant transformations. This can increase the efficiency greatly."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91ec9fda-7848-4f01-8a36-3b82b78be007"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Task1","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4038497947934869}},"nbformat":4,"nbformat_minor":0}
